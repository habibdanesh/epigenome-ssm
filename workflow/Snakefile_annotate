import os
import json
import math
import numpy as np

### config parameters
root_path = config["root_path"]
in_dir = config["in_dir"]
out_dir = config["out_dir"]
debug = config["debug"]
existing_model = config["model"]
n_chunks = config["n_chunks"] # Number of chunks to divide the genome into

### paths
scripts_dir = root_path + "/workflow/scripts"
training_dir = out_dir + "/training"
annotation_dir = out_dir + "/annotation"
features_dir = annotation_dir + "/features"
plots_dir = out_dir + "/plots"
if existing_model is not None:
    model_json = existing_model
else:
    model_json = training_dir + "/model.json"

### model parameters
with open(model_json, 'r') as model_f:
    model_params = json.load(model_f)
    n_features = model_params['K']

### track and file names
n_bins_file = in_dir + "/regions_n_bins.bg"
with open(n_bins_file, 'r') as n_bins_f:
    lines = n_bins_f.readlines()
total_bins = 0
for line in lines:
    total_bins += int(line.strip("\n").split("\t")[-1])
chunk_out_npy = expand("{d}/chunk_{chunk_idx}.npy", d=features_dir, chunk_idx=[i for i in range(n_chunks)])
max_bins = math.ceil(total_bins / n_chunks) # Max. number of bins per chunk
print(f"### total_bins: {total_bins}")
print(f"### n_chunks: {n_chunks}")
print(f"### max_bins: {max_bins}")
##
in_files_locator = in_dir + "/file_locator.tsv"
tracks = []
track_in_files = []
with open(in_files_locator, 'r') as in_f_locator:
    for line in in_f_locator.readlines():
        columns = line.strip("\n").split("\t")
        epigenome = columns[0]
        assay = columns[1]
        path = columns[2]
        track = "{}_{}".format(epigenome, assay)
        tracks.append(track)
        track_in_files.append(path)
feature_npy_files = ["{}/f{}.npy".format(features_dir, f) for f in range(1, n_features+1)]
feature_bw_files = ["{}/f{}.bw".format(features_dir, f) for f in range(1, n_features+1)] # FIXME


### rules
rule all:
    input:
        ## combine_chunk_annotations
        feature_npy_files


rule run_annotation:
    input:
        in_files_locator = in_files_locator,
        model_json = model_json
    params:
        total_bins = total_bins,
        max_bins = max_bins
    output:
        chunk_out_npy = features_dir + "/chunk_{chunk_idx}.npy"
    threads:
        2
    resources:
        mem_mb = 2048
    script:
        scripts_dir + "/run_annotation.py"


rule combine_chunk_annotations:
    input:
        chunk_out_npy = chunk_out_npy
    output:
        feature_npy_files = feature_npy_files
    threads:
        workflow.cores
    run:
        feature_arr = np.empty((n_features, 0), dtype=np.single)
        for i, chunk_npy_file in enumerate(input.chunk_out_npy):
            chunk_idx = int(chunk_npy_file[:-len(".npy")].split('_')[-1])
            assert i == chunk_idx
            chunk_arr = np.load(chunk_npy_file)
            feature_arr = np.concatenate((feature_arr, np.load(chunk_npy_file)), axis=1)
        print("\nfeature_arr: {}\n".format(feature_arr.shape)) # shape is K x G
        ### save
        for k in range(n_features):
            np.save(output.feature_npy_files[k], feature_arr[k, :])
        ### clean up
        for chunk_npy_file in input.chunk_out_npy:
            os.remove(chunk_npy_file)