import os
import json
import math
import numpy as np

### config parameters
root_path = config["root_path"]
in_dir = config["in_dir"]
out_dir = config["out_dir"]
debug = config["debug"]
existing_model = config["model"]
n_chunks = config["n_chunks"] # Number of chunks to divide the genome into

### paths
scripts_dir = root_path + "/workflow/scripts"
training_dir = out_dir + "/training"
annotation_dir = out_dir + "/annotation"
features_dir = annotation_dir + "/features"
plots_dir = out_dir + "/plots"
if existing_model is not None:
    model_json = existing_model
else:
    model_json = training_dir + "/model.json"

### model parameters
with open(model_json, 'r') as model_f:
    model_params = json.load(model_f)
    n_features = model_params['K']

### track and file names
bin_regions_file = in_dir + "/regions.bed"
##
n_bins_file = in_dir + "/regions_n_bins.bg"
n_bins_list = []
with open(n_bins_file, 'r') as n_bins_f:
    lines = n_bins_f.readlines()
for line in lines:
    n_bins_list.append(int(line.strip("\n").split("\t")[-1]))
total_bins = sum(n_bins_list)
chunk_npz_files = expand("{d}/chunk_{chunk_idx}.npz", d=features_dir, chunk_idx=[i for i in range(n_chunks)])
max_bins = math.ceil(total_bins / n_chunks) # Max. number of bins per chunk
print(f"### total_bins: {total_bins}")
print(f"### n_chunks: {n_chunks}")
print(f"### max_bins: {max_bins}")
##
in_files_locator = in_dir + "/file_locator.tsv"
tracks = []
track_in_files = []
with open(in_files_locator, 'r') as in_f_locator:
    for line in in_f_locator.readlines():
        columns = line.strip("\n").split("\t")
        epigenome = columns[0]
        assay = columns[1]
        path = columns[2]
        track = "{}_{}".format(epigenome, assay)
        tracks.append(track)
        track_in_files.append(path)
feature_npz_files = ["{}/f{}.npz".format(features_dir, f) for f in range(1, n_features+1)]
feature_bw_files = ["{}/f{}.bigwig.gz".format(features_dir, f) for f in range(1, n_features+1)]


### rules
rule all:
    input:
        ## combine_chunk_annotations
        feature_npz_files


rule run_annotation:
    input:
        in_files_locator = in_files_locator,
        model_json = model_json
    params:
        total_bins = total_bins,
        max_bins = max_bins
    output:
        chunk_npz_file = features_dir + "/chunk_{chunk_idx}.npz"
    threads:
        2
    resources:
        mem_mb = 4000
    script:
        scripts_dir + "/run_annotation.py"


rule combine_chunk_annotations:
    input:
        chunk_npz_files = chunk_npz_files
    output:
        feature_npz_files = feature_npz_files
    threads:
        workflow.cores
    run:
        feature_mat = np.matrix([], dtype=np.single)
        for i, chunk_npz_file in enumerate(input.chunk_npz_files):
            chunk_idx = int(chunk_npz_file[:-len(".npz")].split('_')[-1])
            assert i == chunk_idx
            chunk_mat = np.load(chunk_npz_file)["arr_0"]
            feature_mat = np.hstack([feature_mat, chunk_mat]) if feature_mat.size else chunk_mat
        print("\nfeature_mat: {}\n".format(feature_mat.shape)) # shape is K x G
        ### save
        for k in range(n_features):
            np.savez_compressed(output.feature_npz_files[k], feature_mat[k, :])
        ### clean up
        for chunk_npz_file in input.chunk_npz_files:
            os.remove(chunk_npz_file)